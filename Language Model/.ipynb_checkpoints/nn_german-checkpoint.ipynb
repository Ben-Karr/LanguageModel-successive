{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# German ULMFiT from scratch\n",
    "The original (backbone of this) notebook is from the fastai-nlp course. Because of memory issues, the wiki text-files (from the get_wiki function) is (manually) split into manageable batches and loaded/learned on individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'de'\n",
    "name = f'{lang}wiki'\n",
    "\n",
    "path = Path('data/dewiki')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nlputils import split_wiki,get_wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/dewiki/dewiki already exists; not downloading\n"
     ]
    }
   ],
   "source": [
    "get_wiki(path,lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#! head -n4 {path}/{name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/dewiki/docs already exists; not splitting\n"
     ]
    }
   ],
   "source": [
    "dest = split_wiki(path,lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Setup paths, hyperparams and vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = path / 'docs'\n",
    "mdl_path = path/'models'\n",
    "\n",
    "path.mkdir(exist_ok=True, parents=True)\n",
    "mdl_path.mkdir(exist_ok=True)\n",
    "\n",
    "folders = 'A B C D E F G H I J K L M N O P Q R S1 S2 T U V W X Y Z char digit'.split()\n",
    "\n",
    "lr = 1e-2\n",
    "bs =  64\n",
    "lr *= bs/48  # Scale learning rate by batch size\n",
    "\n",
    "tmp_vocab = None\n",
    "lm_fns = ['tmp','tmp_vocab']\n",
    "done = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for k,folder in enumerate(folders):\n",
    "    # skip already used folders\n",
    "    if folder in done:\n",
    "        continue\n",
    "        \n",
    "    current_path = doc_path / folder\n",
    "    print('Load DataBunch from: ', current_path)\n",
    "    \n",
    "    # load the (text) files from the current folder\n",
    "    data = (TextList.from_folder(current_path, vocab = tmp_vocab)\n",
    "            .split_by_rand_pct(0.1, seed = 42)\n",
    "            .label_for_lm()           \n",
    "            .databunch(bs = bs, num_workers = 0))\n",
    "    \n",
    "    # check if pretrained weights exist\n",
    "    if (mdl_path / (lm_fns[0] + '.pth')).is_file() and (mdl_path / (lm_fns[1] + '.pkl')).is_file():\n",
    "        print('Create Learner with pretrained weights')\n",
    "        # create the learner with previously trained weights\n",
    "        learn = language_model_learner(data, \n",
    "                                       AWD_LSTM, \n",
    "                                       drop_mult = 0.5,\n",
    "                                       path = path,\n",
    "                                       pretrained_fnames = lm_fns).to_fp16()\n",
    "    else:\n",
    "        # create the learner for first batch\n",
    "        learn = language_model_learner(data, \n",
    "                                       AWD_LSTM, \n",
    "                                       drop_mult = 0.5, \n",
    "                                       pretrained = False).to_fp16()\n",
    "    \n",
    "    # learn on current batch\n",
    "    learn.unfreeze()\n",
    "    learn.fit_one_cycle(1, lr, moms=(0.8,0.7))\n",
    "    \n",
    "    # save weights and vocab    \n",
    "    print('Save LM learner at: ', current_path)\n",
    "    learn.to_fp32().save(mdl_path.resolve() / lm_fns[0], with_opt=False)   \n",
    "    \n",
    "    tmp_vocab = learn.data.vocab\n",
    "    tmp_vocab.save(mdl_path.resolve() / (lm_fns[1] + '.pkl'))\n",
    "    \n",
    "    done.append(folder)\n",
    "    \n",
    "    # backup\n",
    "    if (k % 10) == 0:\n",
    "        fn = 'backup_at_' + str(folder)\n",
    "        learn.save(fn)\n",
    "    \n",
    "    # release GPU memory\n",
    "    del(data)\n",
    "    del(learn)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradually_learning(folders, bs = 64, lr = 1e-2, bwd = False, sentence_piece = False):\n",
    "    \n",
    "    lr = lr\n",
    "    bs =  bs\n",
    "    lr *= bs/48  # Scale learning rate by batch size\n",
    "    quotemark = ''\n",
    "\n",
    "    \n",
    "    tmp_vocab = None\n",
    "    lm_fns = ['tmp','tmp_vocab']\n",
    "    done = []\n",
    "    \n",
    "    if sentence_piece:\n",
    "        proc = [OpenFileProcessor(), SPProcessor()] # processor for sentencepiece\n",
    "        lm_fns = [fn + '_SP' for fn in lm_fns]\n",
    "    else:\n",
    "        proc = None\n",
    "        \n",
    "    if bwd:\n",
    "        lm_fns = [fn + '_bwd' for fn in lm_fns]\n",
    "    \n",
    "    for k,folder in enumerate(folders):\n",
    "        # skip already used folders\n",
    "        if folder in done:\n",
    "            continue\n",
    "        folder = 'char'\n",
    "\n",
    "        current_path = doc_path / folder\n",
    "        print('Load DataBunch from: ', current_path)\n",
    "\n",
    "        # load the (text) files from the current folder\n",
    "        data = (TextList.from_folder(current_path, vocab = tmp_vocab, processor = proc)\n",
    "                .split_by_rand_pct(0.1, seed = 42)\n",
    "                .label_for_lm()           \n",
    "                .databunch(bs = bs, num_workers = 0, backwards = bwd))\n",
    "\n",
    "        # check if pretrained weights exist\n",
    "        if (mdl_path / (lm_fns[0] + '.pth')).is_file() and (mdl_path / (lm_fns[1] + '.pkl')).is_file():\n",
    "            print('Create Learner with pretrained weights')\n",
    "            # create the learner with previously trained weights\n",
    "            learn = language_model_learner(data, \n",
    "                                           AWD_LSTM, \n",
    "                                           drop_mult = 0.5,\n",
    "                                           path = path,\n",
    "                                           pretrained_fnames = lm_fns).to_fp16()\n",
    "        else:\n",
    "            # create the learner for first batch\n",
    "            learn = language_model_learner(data, \n",
    "                                           AWD_LSTM, \n",
    "                                           drop_mult = 0.5, \n",
    "                                           pretrained = False).to_fp16()\n",
    "\n",
    "        # learn on current batch\n",
    "        print('Start learning')\n",
    "        learn.unfreeze()\n",
    "        learn.fit_one_cycle(1, lr, moms=(0.8,0.7))\n",
    "        \n",
    "        break\n",
    "        # save weights and vocab    \n",
    "        print('Save LM learner')\n",
    "        learn.to_fp32().save(mdl_path.resolve() / lm_fns[0], with_opt=False)   \n",
    "\n",
    "        tmp_vocab = learn.data.vocab\n",
    "        tmp_vocab.save(mdl_path.resolve() / (lm_fns[1] + '.pkl'))\n",
    "\n",
    "        done.append(folder)\n",
    "\n",
    "        # backup\n",
    "        if (k % 10) == 0:\n",
    "            fn = 'backup_at_' + str(folder)\n",
    "            learn.save(fn)\n",
    "\n",
    "        # release GPU memory\n",
    "        del(data)\n",
    "        del(learn)\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load DataBunch from:  data/dewiki/docs/char\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start learning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.282540</td>\n",
       "      <td>4.127473</td>\n",
       "      <td>0.398712</td>\n",
       "      <td>03:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gradually_learning(folders, bs = 32, bwd = True, sentence_piece = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
