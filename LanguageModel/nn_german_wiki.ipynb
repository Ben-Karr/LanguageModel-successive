{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# German ULMFiT from scratch\n",
    "The original (backbone of this) notebook is from the fastai-nlp course (see https://github.com/fastai/course-nlp/). Because of memory issues, the wiki text-files (from the get_wiki function) is (manually) split into manageable batches and loaded/learned on individually.\n",
    "\n",
    "The intention is to build several Language Models for German, in different configurations:\n",
    "* forwards or backwards\n",
    "* with or without subword tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'de'\n",
    "name = f'{lang}wiki'\n",
    "\n",
    "path = Path('data/dewiki')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#from nlputils import split_wiki,get_wiki\n",
    "\n",
    "#get_wiki(path,lang)\n",
    "#! head -n4 {path}/{name}\n",
    "#dest = split_wiki(path,lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Setup paths, hyperparams and vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_path = path / 'docs'\n",
    "mdl_path = path / 'models'\n",
    "\n",
    "path.mkdir(exist_ok=True, parents=True)\n",
    "mdl_path.mkdir(exist_ok=True)\n",
    "\n",
    "folders = 'char digit A B C D E F G H I J K L M N O P Q R S1 S2 T U V W X Y Z'.split()\n",
    "\n",
    "lr = 1e-2\n",
    "bs =  64\n",
    "lr *= bs/48  # Scale learning rate by batch size\n",
    "\n",
    "tmp_vocab = None\n",
    "lm_fns = ['tmp','tmp_vocab']\n",
    "done = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for k,folder in enumerate(folders):\n",
    "#    # skip already used folders\n",
    "#    if folder in done:\n",
    "#        continue\n",
    "#        \n",
    "#    current_path = doc_path / folder\n",
    "#    print('Load DataBunch from: ', current_path)\n",
    "#    \n",
    "#    # load the (text) files from the current folder\n",
    "#    data = (TextList.from_folder(current_path, vocab = tmp_vocab)\n",
    "#            .split_by_rand_pct(0.1, seed = 42)\n",
    "#            .label_for_lm()           \n",
    "#            .databunch(bs = bs, num_workers = 0))\n",
    "#    \n",
    "#    # check if pretrained weights exist\n",
    "#    if (mdl_path / (lm_fns[0] + '.pth')).is_file() and (mdl_path / (lm_fns[1] + '.pkl')).is_file():\n",
    "#        print('Create Learner with pretrained weights')\n",
    "#        # create the learner with previously trained weights\n",
    "#        learn = language_model_learner(data, \n",
    "#                                       AWD_LSTM, \n",
    "#                                       drop_mult = 0.5,\n",
    "#                                       path = path,\n",
    "#                                       pretrained_fnames = lm_fns).to_fp16()\n",
    "#    else:\n",
    "#        # create the learner for first batch\n",
    "#        learn = language_model_learner(data, \n",
    "#                                       AWD_LSTM, \n",
    "#                                       drop_mult = 0.5, \n",
    "#                                       pretrained = False).to_fp16()\n",
    "#    \n",
    "#    # learn on current batch\n",
    "#    learn.unfreeze()\n",
    "#    learn.fit_one_cycle(1, lr, moms=(0.8,0.7))\n",
    "#    \n",
    "#    # save weights and vocab    \n",
    "#    print('Save LM learner at: ', current_path)\n",
    "#    learn.to_fp32().save(mdl_path.resolve() / lm_fns[0], with_opt=False)   \n",
    "#    \n",
    "#    tmp_vocab = learn.data.vocab\n",
    "#    tmp_vocab.save(mdl_path.resolve() / (lm_fns[1] + '.pkl'))\n",
    "#    \n",
    "#    done.append(folder)\n",
    "#    \n",
    "#    # backup\n",
    "#    if (k % 10) == 0:\n",
    "#        fn = 'backup_at_' + str(folder)\n",
    "#        learn.save(fn)\n",
    "#    \n",
    "#    # release GPU memory\n",
    "#    del(data)\n",
    "#    del(learn)\n",
    "#    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradually_learning(folders, bs = 64, lr = 1e-2, bwd = False, sentence_piece = False):\n",
    "    \n",
    "    lr = lr\n",
    "    bs =  bs\n",
    "    lr *= bs/48  # Scale learning rate by batch size\n",
    "    \n",
    "    tmp_vocab = None\n",
    "    lm_fns = ['tmp','tmp_vocab']\n",
    "    done = []\n",
    "    \n",
    "    if sentence_piece:\n",
    "        proc = [OpenFileProcessor(), SPProcessor()] # processor for sentencepiece\n",
    "        lm_fns = [fn + '_SP' for fn in lm_fns]\n",
    "    else:\n",
    "        proc = None\n",
    "        \n",
    "    if bwd:\n",
    "        lm_fns = [fn + '_bwd' for fn in lm_fns]\n",
    "    \n",
    "    for k,folder in enumerate(folders):\n",
    "        # skip already used folders\n",
    "        if folder in done:\n",
    "            continue\n",
    "\n",
    "        current_path = doc_path / folder\n",
    "        print('Load DataBunch from: ', current_path)\n",
    "\n",
    "        # load the (text) files from the current folder\n",
    "        data = (TextList.from_folder(current_path, vocab = tmp_vocab, processor = proc)\n",
    "                .split_by_rand_pct(0.1, seed = 42)\n",
    "                .label_for_lm()           \n",
    "                .databunch(bs = bs, num_workers = 0, backwards = bwd))\n",
    "\n",
    "        # check if pretrained weights exist\n",
    "        if (mdl_path / (lm_fns[0] + '.pth')).is_file() and (mdl_path / (lm_fns[1] + '.pkl')).is_file():\n",
    "            print('Create Learner with pretrained weights')\n",
    "            # create the learner with previously trained weights\n",
    "            learn = language_model_learner(data, \n",
    "                                           AWD_LSTM,\n",
    "                                           drop_mult = 0.5,\n",
    "                                           path = path,\n",
    "                                           pretrained_fnames = lm_fns).to_fp16()\n",
    "        else:\n",
    "            # create the learner for first batch\n",
    "            learn = language_model_learner(data, \n",
    "                                           AWD_LSTM, \n",
    "                                           drop_mult = 0.5, \n",
    "                                           pretrained = False).to_fp16()\n",
    "\n",
    "        # learn on current batch\n",
    "        print('Start learning')\n",
    "        learn.unfreeze()\n",
    "        learn.fit_one_cycle(1, lr, moms=(0.8,0.7))\n",
    "        \n",
    "        # save weights and vocab    \n",
    "        print('Save LM learner')\n",
    "        learn.to_fp32().save(mdl_path.resolve() / lm_fns[0], with_opt=False)   \n",
    "\n",
    "        tmp_vocab = learn.data.vocab\n",
    "        tmp_vocab.save(mdl_path.resolve() / (lm_fns[1] + '.pkl'))\n",
    "\n",
    "        done.append(folder)\n",
    "\n",
    "        # backup\n",
    "        if (k % 10) == 0:\n",
    "            fn = 'backup_at_' + str(folder)\n",
    "            learn.save(fn)\n",
    "\n",
    "        # release GPU memory\n",
    "        del(data)\n",
    "        del(learn)\n",
    "        torch.cuda.empty_cache()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradually_learning(folders, bs = 32, bwd = True, sentence_piece = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(pickle.load( open(mdl_path / 'de_fwd_sent_vocab.pkl', \"rb\" )))\n",
    "proc = [OpenFileProcessor(), SPProcessor()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = (TextList.from_folder(doc_path / 'char', vocab = vocab, processor = proc)\n",
    "        .split_by_rand_pct(0.1, seed = 42)\n",
    "        .label_for_lm()           \n",
    "        .databunch(bs = bs, num_workers = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ha ▁in ▁der ▁xxmaj ▁ li schau er ▁xxmaj ▁ schwelle . ▁xxmaj ▁am ▁nordöstlichen ▁xxmaj ▁ortsausgang ▁erstreckt ▁sich ▁der ▁xxmaj ▁teich ▁xxmaj ▁žim ut ický ▁rybník ▁mit ▁der ▁xxmaj ▁mühle ▁xxmaj ▁žim ut ický ▁mlýn ▁unter halb ▁des ▁xxmaj ▁ damm es , ▁südlich ▁liegen ▁der ▁xxmaj ▁far ský ▁rybník ▁und ▁der ▁xxmaj ▁m n ich ovec . ▁xxmaj ▁im ▁xxmaj ▁osten ▁erhebt ▁sich ▁der ▁xxmaj ▁so bě t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>▁xxmaj ▁dabei ▁wurde ▁er ▁1965 ▁und ▁1967 ▁mit ▁der ▁xxmaj ▁mannschaft ▁xxmaj ▁fünfter . ▁xxmaj ▁bei ▁der ▁xxmaj ▁europa meisterschaft ▁1966 ▁erreichte ▁er ▁im ▁xxmaj ▁einzel ▁nach ▁xxmaj ▁siegen ▁über ▁xxmaj ▁ist van ▁xxmaj ▁kor pa ▁( ju go slaw ien ), ▁xxmaj ▁felix ▁xxmaj ▁ fel ten ▁( l ux em burg ), ▁xxmaj ▁pen ti i ▁xxmaj ▁tu o minen ▁( fin n land ), ▁xxmaj ▁stuart ▁xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>▁leben ▁des ▁xxmaj ▁volkes ▁zu ▁richten ▁habe , ▁würden ▁erneut ▁zur ▁xxmaj ▁ disposition ▁gestellt . ▁xxmaj ▁aufgrund ▁des ▁xxmaj ▁todes ▁xxmaj ▁johannes ▁xxmaj ▁paul s ▁xxup ▁i . , ▁welcher ▁der ▁xxmaj ▁konferenz ▁hätte ▁vor stehen ▁sollen , ▁wurde ▁sie ▁vom ▁xxmaj ▁oktober ▁auf ▁den ▁xxmaj ▁januar ▁1979 ▁verschoben ▁und ▁von ▁xxmaj ▁johannes ▁xxmaj ▁paul ▁xxup ▁ii . ▁eröffnet . ▁xxmaj ▁rom ero ▁nutzte ▁diese ▁xxmaj ▁zeit , ▁um</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>▁xxmaj ▁ukraine ▁zu ▁verein igen . ▁xxmaj ▁die ▁xxmaj ▁synode ▁der ▁russisch - orthodoxen ▁xxmaj ▁kirche ▁erklärte ▁daraufhin ▁am ▁15. ▁xxmaj ▁oktober ▁2018 , ▁einseitig ▁die ▁xxmaj ▁ gottesdienst gemeinschaft ▁mit ▁dem ▁xxmaj ▁ökumenische n ▁xxmaj ▁patriarchat ▁abzu brechen . ▁xxmaj ▁zum ▁xxmaj ▁ökumenische n ▁xxmaj ▁patriarchat ▁von ▁xxmaj ▁konstantinopel ▁gehören ▁sechs ▁xxmaj ▁er z diözese n , ▁18 ▁weitere ▁xxmaj ▁metropoli en ▁und ▁acht ▁xxmaj ▁teil kirchen ▁auf ▁allen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>▁kann ▁während ▁der ▁xxmaj ▁öffnung s zeiten ▁des ▁xxmaj ▁museums ▁besichtigt ▁werden . ▁&lt; ▁/ ▁doc &gt; ▁xxbos ▁xxmaj ▁ ḫumbaba ▁xxmaj ▁ ḫumbaba ▁( sprich ▁xxmaj ▁ chum baba ), ▁früh ▁auch ▁xxmaj ▁hu wa wa , ▁ist ▁in ▁der ▁sumerisch en ▁xxmaj ▁mythologie ▁der ▁xxmaj ▁ wächter ▁des ▁xxmaj ▁ ze der n wald es ▁am ▁xxmaj ▁libanon . ▁xxmaj ▁er ▁kommt ▁in ▁mehreren ▁sumerisch en ▁xxmaj ▁my</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_fns = ['de_fwd_spacy','de_fwd_spacy_vocab']\n",
    "\n",
    "learn_lm = language_model_learner(data, \n",
    "                               AWD_LSTM, \n",
    "                               drop_mult = 0.5,\n",
    "                               path = path,\n",
    "                               pretrained_fnames = lm_fns).to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mein Kind Split meint op . 06 : „ Das Fehlen von Schönheit und Lügen der nationalen Kolonialmacht belegten sie energisch über viele Jahre geschlossen an . “ \\n \\n  Jesus : klassischer Vertreter des Videos ist sein halber eigener Kontrast . Er entschied , dass es sich nicht um eine fehlende kompakte Frau handelt , sondern Evelyn , wovon Insgesamt Sebastian Heinrich an die Box erinnert . Entgegen den jeweiligen Mitteln der Formen gilt Ignaz weil auch Elizabeth Buch , die durch ein Lied mit Büchern den Urteil verabschiedet hat . Süd ist hierbei die Fragen eines dünnen Zyklus auf die Menschheit nach Anlehnung an beiden Grenzen und des zahllosen Marokko - enthalten , woraufhin sich die erste Fassung an ihrem \" Verlag \" von Fritsch entwickelt ( u. a. \" Unser Sekunden des Herrn \" von Ferdinand Fischer ) . Eine zweite Version der Fernsehserie beinhaltet allerdings eine neue , einzigartige Aufführung basierend auf ihrer'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_lm.predict('Mein Kind', 200, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(60000, 400, padding_idx=1)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(400, 1152, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(1152, 1152, batch_first=True)\n",
       "      )\n",
       "      (2): WeightDropout(\n",
       "        (module): LSTM(1152, 400, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "      (2): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=400, out_features=60000, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn_lm.model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
